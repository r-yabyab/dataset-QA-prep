Start

Create template: {"Question": "", "Answer": ""}
    - Append Answer when chunking, less than half the max_seq_length
    - check for null, check for EOS within Answer



play around with the token size for A. 400 is kinda big, liek a couple of methods 
- to check how much data is just "{"
- Map files that are done
- Separate API docs


Use cosine_similarity to find Answers that are similar, like default Index.jsx, to remove duplicates
- microsoft/codebert-base

Logging?

How to keep downstream context?

https://github.com/microsoft/CodeBERT
- decoders for cosine_similarity
- graphcodebert
==> these need to be finetuned to work
    - https://github.com/microsoft/CodeBERT/tree/master/UniXcoder#2-similarity-between-code-and-nl
    - use UniXcoder, pretrained on codebases, no NL


No empty files
No dupes (based on semantics)
    - what mean by semantics? can have different ways to write a code that does the same thing? Or more of structure?
No nulls in API docs, txt, anything non code
- how to deal with false positives /negatives?

How to separate dev and prod?


https://platform.openai.com/docs/api-reference/introduction
- for synthdatagen
    https://platform.openai.com/usage

mention language in q?


Best practices
https://platform.openai.com/docs/guides/rft-use-cases
    - NL to structured code that must pass deterministic tests
https://platform.openai.com/docs/guides/fine-tuning-best-practices
    - general finetuning dataset best practices (on openai platform, but applies elsewhere)


Logging
- log on half, then full
- then half w/ eval, then full with eval
    Log starting loss, end loss, 

Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 2.3386, 'grad_norm': 0.40640297532081604, 'learning_rate': 0.0, 'epoch': 0.01}

- try wandb

- how to deal with edge cases at inference?
    - should be caught by tests
    - examples like deprecated, no longer in use classes from 3rd party APIs


- do we need tracing?
    - https://milvus.io/ai-quick-reference/how-do-i-test-and-debug-langchain-applications


https://www.reddit.com/r/LocalLLaMA/comments/1aujfiz/real_free_alternative_to_langsmith/


tree-sitter
- is not a parser, but a parser generator to generate parsers for diff programming languages
    - to do this, needs grammar which is in here
    https://tree-sitter.github.io/tree-sitter/7-playground.html
        - playground

To do:
- use actual java repo
- split on functions with AST parser
- finetune on half and full both with training and eval
    - compare eval loss as a metric on learning
    - tree-sitter for basic syntax checking, but for type-errors and variables, use an actual linter (docker images?)
        - generate smoke tests from GPT-5 based on langs, when have finetuned, run half and full inference based on these prompts.
            - Generate all outputs, once get outputs, run all outputs through linting. This is the test suite.
- turn Q&A templating into pynb