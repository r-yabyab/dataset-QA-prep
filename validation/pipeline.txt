Dev:
Goal: generate code at inference based on my github repositories
Problem: Different programming languages, static parsing

1. Split each file into less than 500 tokens (llama-7b) into A in Q by:
    a. Lang separators through LangChain (code snippets, not validated)
    b. AST
        requires separate parsers for each language
        https://github.com/github/CodeSearchNet?tab=readme-ov-file#data
        - split into 2m (comment, code): 
            - comments are top-level fx and method comments (like python docstring)
            - code is entire fx / method
        https://arxiv.org/pdf/1909.09436 (Semantic Code Search paper)
        - Filters:
            - comments less than 3 token filtered
            - fx names with substring "test"
        https://huggingface.co/datasets/AhmedSSoliman/CodeSearchNet-Python
        - example datasets for python in (code, doc text)
        https://huggingface.co/datasets/sentence-transformers/codesearchnet
        - various langs, (comment, code)
        https://huggingface.co/datasets/claudios/code_search_net
        - reupload of 2m dataset for python
        S3 bucket datasets from CodeSearchNet could be found like this:
        - ds = load_dataset("code_search_net", "python")
        - https://github.com/github/CodeSearchNet/blob/master/notebooks/ExploreData.ipynb

2. Filter
    a. Small answers 
    b. Sensitive material
    c. Dupes (through code embedding then cosine_sim)
3. Generate synthetic Qs
    a. Based on Answer alone
    b. Based on Answer + RAG context (pass Answer, no LLM if possible?) to maintain context
        --> Answer (source code snippet) + context
4. Template to LLM
5. Split into train and eval datasets
    - default params on full v. half size dataset, then change params
    - use logger (wandb)

---
Inference Validation
    a. Static parse
    b. Run code snippet
        ==> need to pass through docker containers maybe


---


